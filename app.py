# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11uobFb7hHTUTD2ABgMj15NLKDovV6C_o
"""

!sudo apt install tesseract-ocr
!pip install pytesseract

!pip install flask

import cv2
import pytesseract
import os
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np
import networkx as nx
import warnings
warnings.filterwarnings("ignore")
from PIL import Image
from sklearn.naive_bayes import GaussianNB
from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.externals import  joblib
import sys
from flask import Flask,request,jsonify
import flask
from sklearn.externals import joblib
app = Flask(__name__)
from PIL import Image
import base64
import io

def read_article(file_name):
    file = open(file_name, "r")
    filedata = file.readlines()
    article = filedata[0].split(". ")
    sentences = []

    for sentence in article:
        #print(sentence)
        sentences.append(sentence.replace("[^a-zA-Z]", " ").split(" "))
    sentences.pop() 
    
    return sentences

def sentence_similarity(sent1, sent2, stopwords=None):
    if stopwords is None:
        stopwords = []
 
    sent1 = [w.lower() for w in sent1]
    sent2 = [w.lower() for w in sent2]
 
    all_words = list(set(sent1 + sent2))
 
    vector1 = [0] * len(all_words)
    vector2 = [0] * len(all_words)
 
    # build the vector for the first sentence
    for w in sent1:
        if w in stopwords:
            continue
        vector1[all_words.index(w)] += 1
 
    # build the vector for the second sentence
    for w in sent2:
        if w in stopwords:
            continue
        vector2[all_words.index(w)] += 1
 
    return 1 - cosine_distance(vector1, vector2)

def build_similarity_matrix(sentences, stop_words):
    # Create an empty similarity matrix
    similarity_matrix = np.zeros((len(sentences), len(sentences)))
 
    for idx1 in range(len(sentences)):
        for idx2 in range(len(sentences)):
            if idx1 == idx2: #ignore if both are same sentences
                continue 
            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)

    return similarity_matrix

valid_list=[]
with open('valid_file.txt') as f:
    valid_list = f.readlines()
valid_labels=[1]*len(valid_list)
print(len(valid_labels))

invalid_list=[]
with open('invalid_file.txt') as f:
    invalid_list = f.readlines()
invalid_labels=[0]*len(invalid_list)
print(len(invalid_labels))

data = open('corpus').read()
labels, texts = [], []
for i, line in enumerate(data.split("\n")):
    content = line.split()
    labels.append(0)
    texts.append(" ".join(content[1:]))
    if(i==99):
      break

invalid_list=invalid_list+texts
invalid_labels=invalid_labels+labels

print(len(invalid_list))
print(len(invalid_labels))

input_data=valid_list+invalid_list
output_data=valid_labels+invalid_labels

print(len(input_data))
print(len(output_data))

data = pd.DataFrame()
data['text'] = input_data
data['label'] = output_data

data.head(5)

x_train, x_test, y_train, y_test = model_selection.train_test_split(data['text'], data['label'], test_size = 0.3)
encoder = preprocessing.LabelEncoder()
train_y = encoder.fit_transform(y_train)
valid_y = encoder.fit_transform(y_test)

tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\w{1,}', max_features=5000)
tfidf_vect.fit(data['text'])

def preprocess(img):
  out_below = pytesseract.image_to_string(img)
  out_below=out_below.split('\n')
  out_below=' '.join(out_below)
  with open('exp.txt', 'w') as writefile:
    writefile.write(out_below)
  summary=generate_summary( "exp.txt")
  data.loc[len(data.index)]=[summary,0]
  summary_tf_idf =  tfidf_vect.transform(data['text'])
  summary_tf_idf=summary_tf_idf[-1,:]
  data=data.drop(labels=-1,axis=0)
  return summary_tf_idf.toarray()

model = joblib.load("model.pkl")
@app.route("/predict",methods=['POST'])
def predict():
    payload = request.form.to_dict(flat=False)
    im_b64 = payload['discharge_summary_image'][0]
    im_binary = base64.b64decode(im_b64)
    buf = io.BytesIO(im_binary)
    image = Image.open(buf)
    vector = preprocess(image)
    result = model.predict(vector)

    return jsonify({'is_valid': bool(result)})

if __name__ == '__main__' :
    try:
        port = int(sys.argv[1])
    except:
        port = 1234
        app.run(port=port, debug=True)

